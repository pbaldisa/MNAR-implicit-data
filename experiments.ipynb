{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps\n",
    "In this notebook, we will reproduce the experiments on real data that the original researchers did in their paper. We will also do some additional experiments to see how the results change when we change the parameters of the algorithm.\n",
    "\n",
    "In order to reuse the code from the original repository, we will clone it into the current directory. We will then import the necessary classes and functions from the original code. We will also need to download the data used in the experiments. The data is available [here](https://webscope.sandbox.yahoo.com/catalog.php?datatype=r), you should look for the R3 - Yahoo! Music rating for User Selected and Randomly Selected songs. I have added the relevant files to the `data` directory in this repository.\n",
    "\n",
    "Run the next cell to clone the original repository. Once the repository is cloned, you will be able to find the orignal code in the directory `unbiased-implicit-rec-real`. We should also change the name of the folder to `base_code' to avoid confusion, the following cell also does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'unbiased-implicit-rec-real'...\n",
      "remote: Enumerating objects: 92, done.\u001b[K\n",
      "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 92 (delta 31), reused 68 (delta 17), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (92/92), 479.73 KiB | 3.45 MiB/s, done.\n",
      "Resolving deltas: 100% (31/31), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/usaito/unbiased-implicit-rec-real.git\n",
    "! mv unbiased-implicit-rec-real base_code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should make sure we have the necessary packages installed. Run the next cell to install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.16.2\n",
      "  Downloading numpy-1.16.2.zip (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2.tar.gz (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn==0.20.3\n",
      "  Downloading scikit-learn-0.20.3.tar.gz (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -r unbiased-implicit-rec-real/requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/paubaldillousalse/Documents/RecSys/MNAR-implicit-data/experiments.ipynb Celda 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paubaldillousalse/Documents/RecSys/MNAR-implicit-data/experiments.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbase_code\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n",
      "File \u001b[0;32m~/Documents/RecSys/MNAR-implicit-data/base_code/src/trainer.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mevaluate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m aoa_evaluator\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpomf\u001b[39;00m \u001b[39mimport\u001b[39;00m ExpoMF\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecommenders\u001b[39;00m \u001b[39mimport\u001b[39;00m PointwiseRecommender\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "from base_code.src.trainer import Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 3 models\n",
    "In this next part, we will train 3 models on the data, so that we can compare them.\n",
    "\n",
    "The models are: \n",
    "* Weighted Matrix Factorization (WMF)\n",
    "* Exposure Matrix Factorization (ExpoMF)\n",
    "* Relevance Matrix Factorization (Rel-MF) (the model proposed in the paper)\n",
    "\n",
    "In order to train the models, we will use the ```Trainer``` class from the original repository. The ```Trainer``` takes as input the following parameters:\n",
    "* ```batch_size```: the size of the batches used to train the model\n",
    "* ```max_iters```: the maximum number of iterations to train the model\n",
    "* ```lam```\n",
    "* ```eta```\n",
    "* ```model_name```: the name of the model to train (one of ```wmf```, ```expomf```, ```crmf```)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters\n",
    "We will define the parameters for the models. We will use the same parameters as the original researchers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer parameters\n",
    "batch_size = 15\n",
    "max_iters = 301\n",
    "lam = 0.0001\n",
    "eta = 0.005\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train WMF model\n",
    "This is one of the baselines used in the paper. We will train it on the data and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'wmf'\n",
    "wmf_trainer = Trainer(model_name, batch_size, max_iters, lam, eta)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
